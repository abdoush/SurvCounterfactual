{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f11d5e-939e-437a-8c86-d3d092710cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Dataset.cmapss import CMAPSS\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f03e00-0153-413c-9be1-ab42444064a5",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3663714f-ac14-43c8-9a42-de6c8217b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmapss = CMAPSS(r\"Dataset\\CMAPSSData\\train_FD003.txt\", max_cycle=300, max_rul=None)\n",
    "\n",
    "# tuples with subsets (idx, X, T, E)\n",
    "train, test, val = cmapss.get_train_test_val(test_size=0.2, val_size=0.2, scale=True)\n",
    "\n",
    "# now it only works for FD003\n",
    "failure_modes = cmapss.get_failure_mode(cmapss.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7176301-8170-452d-869b-a86bacb309f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_time_data(subset):\n",
    "    T = subset[2]\n",
    "    E = subset[3]\n",
    "    dtypes = np.dtype('bool,float')\n",
    "    ET = np.array([(bool(e), t) for e, t in zip(E, T)], dtype=dtypes)\n",
    "    \n",
    "    return ET\n",
    "\n",
    "def get_idx_data(subset):\n",
    "    return subset[0]\n",
    "\n",
    "def get_features_data(subset):\n",
    "    return subset[1]\n",
    "\n",
    "def get_time_data(subset):\n",
    "    return subset[2]\n",
    "\n",
    "def get_event_data(subset):\n",
    "    return subset[3]\n",
    "\n",
    "idx_train = get_idx_data(train)\n",
    "X_train = get_features_data(train)\n",
    "ET_train = get_event_time_data(train)\n",
    "\n",
    "idx_test = get_idx_data(test)\n",
    "X_test = get_features_data(test)\n",
    "ET_test = get_event_time_data(test)\n",
    "\n",
    "idx_val = get_idx_data(val)\n",
    "X_val = get_features_data(val)\n",
    "ET_val = get_event_time_data(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a81d3f-2245-43dc-b6fc-f13107beb24c",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c89640-9406-4efd-89dc-b7f01c6ea2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_train)\n",
    "X_pca_train = pca.transform(X_train)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"Data distribution (PCA)\")\n",
    "plt.scatter(X_pca_train[:, 0], X_pca_train[:, 1], c=get_time_data(train), alpha=1, cmap='viridis', s=1, label=\"Train data\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201758f7-a13f-42bf-92c8-acaafb730978",
   "metadata": {},
   "source": [
    "### Survival Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496cc309-7668-4f2e-b5e0-dbf59eac4811",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# survival model\n",
    "survivalForest = RandomSurvivalForest(n_estimators=12, max_depth=5, n_jobs=-1)\n",
    "survivalForest.fit(X_train, ET_train)\n",
    "\n",
    "cindex_train = survivalForest.score(X_train, ET_train)\n",
    "cindex_val = survivalForest.score(X_val, ET_val)\n",
    "cindex_test = survivalForest.score(X_test, ET_test)\n",
    "\n",
    "print('Train cindex {:.3f}'.format(cindex_train))\n",
    "print('Val cindex {:.3f}'.format(cindex_val))\n",
    "print('Test cindex {:.3f}'.format(cindex_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d57248d-119d-402f-a711-1e41dc02ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# survival curves\n",
    "x_time = survivalForest.event_times_\n",
    "surv_train = survivalForest.predict_survival_function(X_train, return_array=True)\n",
    "surv_test = survivalForest.predict_survival_function(X_test, return_array=True)\n",
    "surv_val = survivalForest.predict_survival_function(X_test, return_array=True)\n",
    "\n",
    "class SurvivalScoreModel:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if X.ndim==1:\n",
    "            X = X.reshape((1, -1))\n",
    "        \n",
    "        x_time = self.model.event_times_\n",
    "        survival_curve = self.model.predict_survival_function(X, return_array=True)\n",
    "        \n",
    "        return np.trapz(survival_curve, x_time)\n",
    "\n",
    "survivalScoreModel = SurvivalScoreModel(survivalForest)\n",
    "\n",
    "\n",
    "survival_score_train = survivalScoreModel.predict(X_train)\n",
    "survival_score_test = survivalScoreModel.predict(X_test)\n",
    "survival_score_val = survivalScoreModel.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13169a26-fbff-41db-978b-3f02cc095eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_test = get_time_data(test)\n",
    "E_test = get_event_data(test).astype(bool)\n",
    "failure_test = np.array(list(map(lambda x: failure_modes[x], idx_test)))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(\"Survival Score for test units as a function of time-to-event (only for failure events)\")\n",
    "plt.scatter(T_test[E_test], survival_score_test[E_test], s=8, c=[failure_test[E_test]], cmap='bwr')\n",
    "plt.xlabel(\"Time to event [cycles]\")\n",
    "plt.ylabel(\"Survival Score\")\n",
    "plt.xlim(0, )\n",
    "plt.ylim(0, )\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39993ca4-23c0-412d-8d3c-ce942025e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "units_test = np.unique(idx_test)\n",
    "cycle = 150\n",
    "labeled_0 = False\n",
    "labeled_1 = False\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "for unit in units_test:\n",
    "    X_units = X_test[idx_test == unit][cycle]\n",
    "    surv_unit = surv_test[idx_test == unit][cycle]\n",
    "    mode = failure_modes[unit]\n",
    "    \n",
    "    label = None\n",
    "    \n",
    "    if mode == 0:\n",
    "        color='blue'\n",
    "        if not labeled_0:\n",
    "            label = \"Failure Mode 1\"\n",
    "            labeled_0 = True\n",
    "    else:\n",
    "        color='red'\n",
    "        if not labeled_1:\n",
    "            label = \"Failure Mode 2\"\n",
    "            labeled_1 = True\n",
    "\n",
    "    plt.plot(x_time, surv_unit, c=color, label=label, alpha=0.8, linewidth=2)\n",
    "    \n",
    "plt.legend()\n",
    "plt.xlim(0, x_time.max())\n",
    "plt.xlabel(\"Number of Future Cycles\")\n",
    "plt.ylabel(\"Survival Porbability\")\n",
    "plt.title(\"Survival Curves for test units at t=%i\" % cycle)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4694b055-8d9f-40bb-998b-6170a0b5bf17",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9780335-d5e8-4970-8a51-c2f093a29c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder import Autoencoder, AutoencoderDataset, AutoencoderLearner\n",
    "import torch\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "LOAD_WEIGHTS = True\n",
    "RUN_TRAINING = False\n",
    "SAVE_WEIGHTS = False\n",
    "weights_checkpoint = 'checkpoint.pt'\n",
    "\n",
    "\n",
    "train_loader = AutoencoderDataset(X_train)\n",
    "val_loader = AutoencoderDataset(X_val)\n",
    "\n",
    "anomaly_model = Autoencoder(n_features=X_train.shape[-1],\n",
    "                         hidden_layers_size=[6],\n",
    "                         latent_size=12,\n",
    "                         activation=\"relu\",\n",
    "                         last_activation=\"sigmoid\")\n",
    "\n",
    "if LOAD_WEIGHTS:\n",
    "    anomaly_model.load_weights(weights_checkpoint)\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    optimizer = torch.optim.Adam(anomaly_model.parameters(), lr=1e-3)\n",
    "    loss_function = MSELoss()\n",
    "\n",
    "    train_loss_list, valid_loss_list = AutoencoderLearner.run_training(anomaly_model, optimizer, loss_function, train_loader, val_loader, epochs=10,\n",
    "                                                                       early_stopping=True, early_stopping_patience=5, early_stopping_delta=1e-4)\n",
    "    \n",
    "    if SAVE_WEIGHTS:\n",
    "        anomaly_model.save_weights(weights_checkpoint)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_loss_list, label=\"Train Loss\")\n",
    "    plt.plot(valid_loss_list, label=\"Test Loss\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733f71f8-6922-4d1c-94af-a07f379da48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is used to calculate anomaly score with and without softplus activation\n",
    "\n",
    "errors = anomaly_model.anomaly_score(X_test)\n",
    "errors_softplus = anomaly_model.anomaly_score(X_test, anomaly_threshold=0.1)\n",
    "plt.hist(errors, bins=60)\n",
    "plt.hist(errors_softplus, bins=60)\n",
    "plt.show()\n",
    "\n",
    "for percentile in (50, 90, 95, 99):\n",
    "    value = np.quantile(errors, percentile / 100)\n",
    "    print(\"%sth percentile = %.3f\" % (percentile, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433d9f37-4c40-46a8-93d1-c64052408f95",
   "metadata": {},
   "source": [
    "### Counterfactual Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decf53d1-b2b8-466c-ba4c-bc628425d2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import counterfactuals\n",
    "import optimization\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def select_best_counterfactuals(counterfactuals_list, n=10):\n",
    "    counterfactuals_list = list(sorted(counterfactuals_list, key=lambda x: x[1]))\n",
    "    counterfactuals_list = counterfactuals_list[:n]\n",
    "    counterfactual_points = list(map(lambda x: x[0], counterfactuals_list))\n",
    "    \n",
    "    return counterfactual_points\n",
    "\n",
    "feature_types = ['float' for i in range(X_train.shape[-1])]\n",
    "\n",
    "explainer = counterfactuals.CounterfactualExplainer(survivalScoreModel, X_train, feature_types, anomaly_model=anomaly_model, anomaly_threshold=0.1,\n",
    "                                    weight_distance=1,\n",
    "                                    weight_target=1,\n",
    "                                    weight_anomaly=5)\n",
    "\n",
    "# this explainer do not take the autoencoder into account\n",
    "explainer_simple = counterfactuals.CounterfactualExplainer(survivalScoreModel, X_train, feature_types, anomaly_model=None,  anomaly_threshold=0.1,\n",
    "                                    weight_distance=1,\n",
    "                                    weight_target=1,\n",
    "                                    weight_anomaly=0 )\n",
    "\n",
    "\n",
    "# selected point\n",
    "x=X_test[200]\n",
    "y = survivalScoreModel.predict(x)\n",
    "\n",
    "\n",
    "step = 0.02\n",
    "y_delta = 0.3\n",
    "x0 = x\n",
    "# target counterfactual value\n",
    "y_target = y * (1 + y_delta)\n",
    "\n",
    "counterfactuals_list = []\n",
    "counterfactuals_simple_list = []\n",
    "\n",
    "print(\"Generating counterfactuals...\")\n",
    "for it in tqdm(range(25)):\n",
    "    #print(\"Iteration\", it)\n",
    "    #optimizer = optimization.SimulatedAnnealing(1, 1e-3, iterations=600, step_decrease_rate=0)\n",
    "    optimizer = optimization.ParticleSwarmOptimization(n_particles=50, patience=10, iterations=800, ftol=-np.inf)\n",
    "    \n",
    "    # counterfactuals with autoencoder\n",
    "    x_exp = explainer.compute_explanation_pso(optimizer, x, x0, y_delta, verbose=False, step=step)\n",
    "    loss = explainer._loss(x_exp, x, y_delta) \n",
    "    counterfactuals_list.append((x_exp, loss))\n",
    "    \n",
    "    # counterfactuals without autoencoder\n",
    "    x_exp = explainer_simple.compute_explanation_pso(optimizer, x, x0, y_delta, verbose=False, step=step)\n",
    "    loss = explainer_simple._loss(x_exp, x, y_delta) \n",
    "    counterfactuals_simple_list.append((x_exp, loss))\n",
    "    \n",
    "    #print(it+1, \"Expected target: %.1f, Counterfactual target: %.1f, Loss: %.3f\" % (y_target, y_exp, loss))\n",
    "    #explainer.print_individual_losses(x, x_exp, 0.1)\n",
    "    # print(\"\")\n",
    "    # add a tuple of (explanation, loss) to previously created list\n",
    "    \n",
    "\n",
    "counterfactual_points = select_best_counterfactuals(counterfactuals_list, n=30)\n",
    "counterfactual_points_simple = select_best_counterfactuals(counterfactuals_simple_list, n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f58a6e-8a7e-483d-afcd-62f77f48eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_base(pca, model, X_base, y_delta, figsize=(10, 7)):\n",
    "    X_base_pca = pca.transform(X_base)\n",
    "    y_base = model.predict(X_base)\n",
    "    y_target = model.predict(x) * (1 + y_delta)\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = plt.subplot()\n",
    "    ax.set_facecolor((0.95, 0.95, 0.95))\n",
    "    base_plot = ax.scatter(X_base_pca[:, 0], X_base_pca[:, 1], c=abs(y_target - y_base), s=5, cmap='viridis', vmin=0, vmax=10, label=\"Base points\", alpha=0.7)\n",
    "    fig.colorbar(base_plot, label=\"Distance to target score\")\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    fig.tight_layout()\n",
    "    #fig.show()\n",
    "    \n",
    "def plot_original_point(x):\n",
    "    x_pca = pca.transform(x.reshape((1, -1)))\n",
    "    plt.scatter(x_pca[:, 0], x_pca[:, 1], s=150, color='black', edgecolor='white', label=\"Original Point\")\n",
    "    \n",
    "\n",
    "def plot_counterfactuals(pca, points, color, label=None):\n",
    "    points = np.array(points)\n",
    "    points_pca = pca.transform(points)\n",
    "    plt.scatter(points_pca[:, 0], points_pca[:, 1], color=color, label=label, edgecolor='white')\n",
    "\n",
    "\n",
    "plot_pca_base(pca, survivalScoreModel, X_test, y_delta)\n",
    "ax = plt.gca()\n",
    "plot_counterfactuals(pca, counterfactual_points, 'blue', 'With AE')\n",
    "plot_counterfactuals(pca, counterfactual_points_simple, 'red', 'Without AE')\n",
    "plot_original_point(x)\n",
    "plt.legend()\n",
    "# plt.xlim(0.25, 0.75)\n",
    "# plt.ylim(-0.4, 0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f3dbd-af16-4564-b511-768943f255f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_counterfactual(explainer, x, x_exp, feature_names=None, sort=False):\n",
    "    delta_x = abs(x_exp - x)\n",
    "    \n",
    "    y = explainer._model.predict(x)\n",
    "    y_exp = explainer._model.predict(x_exp)\n",
    "    \n",
    "    # sort\n",
    "    if sort:\n",
    "        x = [x for _, x in sorted(zip(delta_x, x), key=lambda pair: pair[0])]\n",
    "        x_exp = [x for _, x in sorted(zip(delta_x, x_exp), key=lambda pair: pair[0])]\n",
    "        feature_names = [x for _, x in sorted(zip(delta_x, feature_names), key=lambda pair: pair[0])]\n",
    "        \n",
    "    \n",
    "    yrange = np.arange(len(x))\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.title(\"Original Target=%.1f, Counterfactual Target: %.1f\" % (y, y_exp))\n",
    "    plt.barh(y=yrange+0.15, width=x, height=0.3, label=\"Original Point\", color='black')\n",
    "    plt.barh(y=yrange-0.15, width=x_exp, height=0.3, label=\"Counterfactual\", color='orange')\n",
    "    plt.legend()\n",
    "    plt.ylim(yrange[0]-0.5, yrange[-1] + 0.5)\n",
    "    plt.yticks(ticks=yrange, labels=list(feature_names))\n",
    "    plt.xlabel(\"Normalized feature value\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # \n",
    "\n",
    "x_exp = counterfactual_points[0]\n",
    "plot_counterfactual(explainer, x, x_exp, feature_names=cmapss.df.columns[5:-2], sort=True)\n",
    "x_exp = counterfactual_points[1]\n",
    "plot_counterfactual(explainer, x, x_exp, feature_names=cmapss.df.columns[5:-2], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7714fcc-6fa8-4019-91ca-e679d802f701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f7431e-3cdd-49f5-967a-9dc1845b8fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_arrays(explainer, x, y_delta, counterfactual_points):\n",
    "    loss_target = []\n",
    "    loss_distance = []\n",
    "    loss_anomaly = []\n",
    "    \n",
    "    if x.ndim==1:\n",
    "        x = x.reshape((1, -1))\n",
    "    \n",
    "    for x_exp in counterfactual_points:\n",
    "        y = explainer._model.predict(x)\n",
    "        y_target = y * (1 + y_delta)\n",
    "        y_exp = explainer._model.predict(x_exp)\n",
    "        \n",
    "        lt = explainer._distance_target(y_target, y_exp)\n",
    "        ld = explainer._distance_features(x, x_exp)\n",
    "        la = explainer._anomaly_model.anomaly_score(x_exp.reshape((1, -1)))\n",
    "        \n",
    "        loss_target.append(lt)\n",
    "        loss_distance.append(lt)\n",
    "        loss_anomaly.append(la)\n",
    "        \n",
    "    \n",
    "    return loss_target, loss_distance, loss_anomaly\n",
    "\n",
    "loss_target, loss_distance, loss_anomaly = get_loss_arrays(explainer, x, 0.3, counterfactual_points)\n",
    "loss_target_simple, loss_distance_simple, loss_anomaly_simple = get_loss_arrays(explainer, x, 0.3, counterfactual_points_simple)\n",
    "\n",
    "loss_df = pd.DataFrame()\n",
    "i=0\n",
    "for lt, ld, la in zip(loss_target, loss_distance, loss_anomaly):\n",
    "    loss_df.at[i, \"AE\"] = True\n",
    "    loss_df.at[i, \"Target\"] = lt\n",
    "    loss_df.at[i, \"Distance\"] = ld\n",
    "    loss_df.at[i, \"Anomaly Score\"] = la\n",
    "    i+=1\n",
    "    \n",
    "for lt, ld, la in zip(loss_target_simple, loss_distance_simple, loss_anomaly_simple):\n",
    "    loss_df.at[i, \"AE\"] = False\n",
    "    loss_df.at[i, \"Target\"] = lt\n",
    "    loss_df.at[i, \"Distance\"] = ld\n",
    "    loss_df.at[i, \"Anomaly Score\"] = la\n",
    "    i+=1\n",
    "    \n",
    "loss_df = loss_df.melt(id_vars='AE', var_name='Loss Type')\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.title(\"Distribution of loss value for counterfactuals\")\n",
    "sns.boxplot(data=loss_df, x=\"Loss Type\", y=\"value\", hue=\"AE\", width=0.4, fliersize=2, saturation=1.0)\n",
    "#plt.yscale('log')\n",
    "#plt.ylim(1e-2, 1)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
